{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC)\n",
    "Soft Actor-Critic (SAC) is an off policy algorithm that combines an actor-critic architecture with a maximum entropy framework. It is more sample-efficient and stable than prior algorithms (DDPG, TD3), making it suitable for complex, continuous action spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Entropy\n",
    "Entropy is a measure of randomness or uncertainty in a probability distribution. It is given by:\n",
    "\n",
    "$$\n",
    "H(p) = -\\int p(x) \\log p(x) dx = \\mathbb{E}_{x \\sim p} [-\\log {p(x)}]\n",
    "$$\n",
    "\n",
    "In the context of reinforcement learning, entropy is used to measure the uncertainty or randomness in the policy. By maximizing entropy, the algorithm encourages exploration and prevents premature convergence to suboptimal policies.\n",
    "\n",
    "We augment the reward with an entropy term to encourage exploration:\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{s_t \\sim p(s_t), a_t \\sim \\pi(\\cdot|s_t)} [r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))]\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is a constant that controls the trade-off between the reward and the entropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Network\n",
    "\n",
    "**Network Architecture is as mentioned in the paper\n",
    "\n",
    "SAC uses the clipped double Q-network trick to better estimate the Q function.\n",
    "\n",
    "Additionally, SAC uses a target network, an exponential moving average of the critic network, to provide a more stable estimate of the expected return when updating the Q function. It is updated slowly as a weighted average of the target and critic network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x1 = self.q1(torch.cat([state, action], 1))\n",
    "        x2 = self.q2(torch.cat([state, action], 1))\n",
    "        return x1, x2\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Network\n",
    "\n",
    "**Network Architecture is as mentioned in the paper\n",
    "\n",
    "The policy network learns the mean and standard deviation of the Gaussian distribution over the actions. The mean is the expected action, and the standard deviation controls the uncertainty or randomness in the action distribution.\n",
    "\n",
    "When sampling actions from the policy network, we use the reparameterization trick to allow for gradient descent on the policy network.\n",
    "\n",
    "We use the tanh function to bound the action space to a finite interval (as stated in the original paper).\n",
    "\n",
    "Thus, as shown in the paper, the log probability of the action is given by:\n",
    "\n",
    "$$\n",
    "\\log \\pi(\\mathbf{a}|\\mathbf{s}) = \\log \\mu(\\mathbf{u}|\\mathbf{s}_t) - \\sum_{i=1}^D \\log (1 - \\tanh^2({u}_i))\n",
    "$$\n",
    "\n",
    "Additionally, we scale the action space to ensure that the output of the policy network is within the range of the action space.\n",
    "\n",
    "When returning the scaled action, we have:\n",
    "$$ P(\\cdot | \\pi_\\theta(s)) = ((\\mathcal{N}(tanh(\\mu_{\\theta}(s)), \\sigma_{\\theta}(s)) ) / 2.0) \\times (h - l) + l  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SIG_MIN = -20\n",
    "LOG_SIG_MAX = 2\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        self.action_scale = torch.tensor((action_space.high - action_space.low) / 2.)\n",
    "        self.action_bias = torch.tensor((action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.mean(state)\n",
    "        log_std = self.log_std(state)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        action = normal.rsample() #rsample() samples from the normal distribution using the reparameterization trick\n",
    "        compressed_action = torch.tanh(action)\n",
    "        scaled_action = compressed_action * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(action)\n",
    "        #We add a small constant to the log probability to prevent numerical instability\n",
    "        log_prob = log_prob - torch.log(self.action_scale * (1 - compressed_action.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return scaled_action, log_prob, mean\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic\n",
    "**Hyperparameters are set as mentioned in the paper\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.005\n",
    "\n",
    "(alpha is initially set arbitrarily to 0.5 since it is learned)\n",
    "\n",
    "network architecture: 2 hidden layers with 256 units each\n",
    "\n",
    "optimizer: Adam\n",
    "\n",
    "learning rate: $3 \\cdot 10^{-4}$\n",
    "\n",
    "During each gradient step, we update the parameters by calculating the loss as follows (updating gradients using torch backpropagation):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(Q) = \\mathbb{E}_{s_t, a_t, r_{t+1}, s_{t+1}, d \\sim \\mathcal{D}} [(Q(s_t, a_t) - (r + \\gamma (1-d) (\\text{min}Q_{target}(s_{t+1}, \\pi(s_{t+1}))-\\alpha \\log \\pi(a_{t+1}|s_{t+1}))))^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\pi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon_t \\sim \\mathcal{N}} [\\alpha \\log \\pi(a_t|s_t) - \\text{min}Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\alpha) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} [-\\alpha \\log \\pi(a_t|s_t) - \\alpha \\mathcal{H}_0]\n",
    "$$\n",
    "\n",
    "For the target networks, we use the following update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{Q'} = \\tau \\theta_{Q} + (1-\\tau) \\theta_{Q'}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, num_inputs, action_space, hidden_dim):\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_dim)\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_dim)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.policy = PolicyNetwork(num_inputs, action_space.shape[0], hidden_dim, action_space)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.5\n",
    "        # self.target_entropy = -torch.prod(torch.Tensor(action_space.shape[0]).to(device)).item()\n",
    "        self.target_entropy = -action_space.shape[0]\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "    \n",
    "\n",
    "    #When in inference mode, we just return the mean since we do not want to incorporate randomness\n",
    "    def action(self, state, inference=False):\n",
    "        if isinstance(state, (int, float)):\n",
    "            state = torch.FloatTensor([state]).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "        if inference:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        else:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        return action.detach().numpy()[0]\n",
    "    \n",
    "    def update(self, memory, batch_size):\n",
    "        states, actions, next_states, rewards, dones = memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "        dones = torch.FloatTensor(dones).to(device).unsqueeze(1)\n",
    "        \n",
    "        #update critic\n",
    "        #We use torch.no_grad() here since the target network does not require gradients\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_pi, _ = self.policy.sample(next_states)\n",
    "            q1_target, q2_target = self.critic_target(next_states, next_action)\n",
    "            q_target = torch.min(q1_target, q2_target) - self.alpha * next_log_pi\n",
    "            next_target_Q = rewards + ((1 - dones) * self.gamma * (q_target))\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        q1_loss = F.mse_loss(q1, next_target_Q)\n",
    "        q2_loss = F.mse_loss(q2, next_target_Q)\n",
    "        q_loss = q1_loss + q2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        #update policy\n",
    "        pi, log_pi, _ = self.policy.sample(states)\n",
    "        q1_pi, q2_pi = self.critic(states, pi)\n",
    "        min_q_pi = torch.min(q1_pi, q2_pi)\n",
    "        \n",
    "        policy_loss = (self.alpha * log_pi - min_q_pi).mean()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        #update alpha\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optim.step()\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        #update target network\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "The replay buffer is used to store past experiences. We use a circular buffer to store the experiences, ejecting old data when the buffer is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, next_state, reward, done)\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, next_state, reward, done = map(np.stack, zip(*batch))\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "**Hyperparameters are set as mentioned in the paper.\n",
    "\n",
    "buffer_size = 1000000\n",
    "batch_size = 256\n",
    "\n",
    "We use the Pendulum-v1 environment from OpenAI Gym.\n",
    "\n",
    "When training, sample an action from the policy network, and add the experience to the replay buffer. If the replay buffer has enough experiences, sample a batch of experiences from the replay buffer, and update the critic network, policy network, and entropy coefficient. Then, add the new data to the replay buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs8803drl/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: | Episode Steps 200 | Episode Reward -1288.2334574071106\n",
      "Episode 1: | Episode Steps 200 | Episode Reward -1007.8151008267273\n",
      "Episode 2: | Episode Steps 200 | Episode Reward -1367.0768384295882\n",
      "Episode 3: | Episode Steps 200 | Episode Reward -1062.141091587513\n",
      "Episode 4: | Episode Steps 200 | Episode Reward -1556.5890641205967\n",
      "Episode 5: | Episode Steps 200 | Episode Reward -1400.8870330406196\n",
      "Episode 6: | Episode Steps 200 | Episode Reward -1575.1278704114347\n",
      "Episode 7: | Episode Steps 200 | Episode Reward -1757.998088904695\n",
      "Episode 8: | Episode Steps 200 | Episode Reward -1687.8123303683117\n",
      "Episode 9: | Episode Steps 200 | Episode Reward -1480.7326791162154\n",
      "Episode 10: | Episode Steps 200 | Episode Reward -1081.431146458793\n",
      "Episode 11: | Episode Steps 200 | Episode Reward -1706.3001678310748\n",
      "Episode 12: | Episode Steps 200 | Episode Reward -906.3868484994194\n",
      "Episode 13: | Episode Steps 200 | Episode Reward -1134.7879343273214\n",
      "Episode 14: | Episode Steps 200 | Episode Reward -263.45249393203426\n",
      "Episode 15: | Episode Steps 200 | Episode Reward -897.5387570443822\n",
      "Episode 16: | Episode Steps 200 | Episode Reward -647.6721059583982\n",
      "Episode 17: | Episode Steps 200 | Episode Reward -524.9979247135586\n",
      "Episode 18: | Episode Steps 200 | Episode Reward -134.63152569845604\n",
      "Episode 19: | Episode Steps 200 | Episode Reward -920.0722716128407\n",
      "Episode 20: | Episode Steps 200 | Episode Reward -136.00767302551827\n",
      "Episode 21: | Episode Steps 200 | Episode Reward -250.160689615595\n",
      "Episode 22: | Episode Steps 200 | Episode Reward -4.1046687308121\n",
      "Episode 23: | Episode Steps 200 | Episode Reward -257.6471886783377\n",
      "Episode 24: | Episode Steps 200 | Episode Reward -384.14593392981675\n",
      "Episode 25: | Episode Steps 200 | Episode Reward -130.99185938761156\n",
      "Episode 26: | Episode Steps 200 | Episode Reward -121.36350339272718\n",
      "Episode 27: | Episode Steps 200 | Episode Reward -241.16822653634216\n",
      "Episode 28: | Episode Steps 200 | Episode Reward -244.05316353929553\n",
      "Episode 29: | Episode Steps 200 | Episode Reward -122.80259323149497\n",
      "Episode 30: | Episode Steps 200 | Episode Reward -127.97518560099937\n",
      "Episode 31: | Episode Steps 200 | Episode Reward -249.313053394362\n",
      "Episode 32: | Episode Steps 200 | Episode Reward -128.9752954753739\n",
      "Episode 33: | Episode Steps 200 | Episode Reward -117.16746512795976\n",
      "Episode 34: | Episode Steps 200 | Episode Reward -243.5590004859395\n",
      "Episode 35: | Episode Steps 200 | Episode Reward -250.01769577578136\n",
      "Episode 36: | Episode Steps 200 | Episode Reward -5.460149329086133\n",
      "Episode 37: | Episode Steps 200 | Episode Reward -125.3495701276134\n",
      "Episode 38: | Episode Steps 200 | Episode Reward -897.946234942838\n",
      "Episode 39: | Episode Steps 200 | Episode Reward -129.88491272514574\n",
      "Episode 40: | Episode Steps 200 | Episode Reward -124.50041849828504\n",
      "Episode 41: | Episode Steps 200 | Episode Reward -129.79918334473112\n",
      "Episode 42: | Episode Steps 200 | Episode Reward -129.9348372449387\n",
      "Episode 43: | Episode Steps 200 | Episode Reward -1062.123140839185\n",
      "Episode 44: | Episode Steps 200 | Episode Reward -129.4090000462118\n",
      "Episode 45: | Episode Steps 200 | Episode Reward -1214.9574188557642\n",
      "Episode 46: | Episode Steps 200 | Episode Reward -121.26502850368882\n",
      "Episode 47: | Episode Steps 200 | Episode Reward -481.7768371134623\n",
      "Episode 48: | Episode Steps 200 | Episode Reward -255.35807882822775\n",
      "Episode 49: | Episode Steps 200 | Episode Reward -127.37082840474291\n",
      "Episode 50: | Episode Steps 200 | Episode Reward -237.15498477980847\n",
      "Episode 51: | Episode Steps 200 | Episode Reward -121.4042849454616\n",
      "Episode 52: | Episode Steps 200 | Episode Reward -1.9872808123029817\n",
      "Episode 53: | Episode Steps 200 | Episode Reward -351.83093486497023\n",
      "Episode 54: | Episode Steps 200 | Episode Reward -117.49485381423322\n",
      "Episode 55: | Episode Steps 200 | Episode Reward -122.38450062863723\n",
      "Episode 56: | Episode Steps 200 | Episode Reward -128.93859689490057\n",
      "Episode 57: | Episode Steps 200 | Episode Reward -124.30127259529418\n",
      "Episode 58: | Episode Steps 200 | Episode Reward -125.9479073351532\n",
      "Episode 59: | Episode Steps 200 | Episode Reward -127.76046094414713\n",
      "Episode 60: | Episode Steps 200 | Episode Reward -122.4899711644814\n",
      "Episode 61: | Episode Steps 200 | Episode Reward -4.094893702957096\n",
      "Episode 62: | Episode Steps 200 | Episode Reward -223.2962145661981\n",
      "Episode 63: | Episode Steps 200 | Episode Reward -119.86632545988007\n",
      "Episode 64: | Episode Steps 200 | Episode Reward -123.15501432466135\n",
      "Episode 65: | Episode Steps 200 | Episode Reward -397.2298199648561\n",
      "Episode 66: | Episode Steps 200 | Episode Reward -116.39078986276883\n",
      "Episode 67: | Episode Steps 200 | Episode Reward -126.21591044576975\n",
      "Episode 68: | Episode Steps 200 | Episode Reward -122.91200673814474\n",
      "Episode 69: | Episode Steps 200 | Episode Reward -356.9505435273336\n",
      "Episode 70: | Episode Steps 200 | Episode Reward -116.07422221339009\n",
      "Episode 71: | Episode Steps 200 | Episode Reward -2.7253019986242575\n",
      "Episode 72: | Episode Steps 200 | Episode Reward -123.59527512227962\n",
      "Episode 73: | Episode Steps 200 | Episode Reward -2.3800977169173034\n",
      "Episode 74: | Episode Steps 200 | Episode Reward -352.0279932881702\n",
      "Episode 75: | Episode Steps 200 | Episode Reward -123.33399099459251\n",
      "Episode 76: | Episode Steps 200 | Episode Reward -235.82316858433467\n",
      "Episode 77: | Episode Steps 200 | Episode Reward -117.60024446710828\n",
      "Episode 78: | Episode Steps 200 | Episode Reward -121.23065832513944\n",
      "Episode 79: | Episode Steps 200 | Episode Reward -128.46678202969892\n",
      "Episode 80: | Episode Steps 200 | Episode Reward -126.87448364417233\n",
      "Episode 81: | Episode Steps 200 | Episode Reward -2.2591912960028186\n",
      "Episode 82: | Episode Steps 200 | Episode Reward -128.62003990714157\n",
      "Episode 83: | Episode Steps 200 | Episode Reward -123.2799740960021\n",
      "Episode 84: | Episode Steps 200 | Episode Reward -120.72876190301596\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "env.reset(seed=1)\n",
    "\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, 64)\n",
    "memory = ReplayBuffer(1000000, 1)\n",
    "batch_size = 256\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(200):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "        action = agent.action(state, inference=False)\n",
    "        if len(memory) > batch_size:\n",
    "            agent.update(memory, batch_size)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "\n",
    "        memory.add(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode {i}: | Episode Steps {episode_steps} | Episode Reward {episode_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rewards over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = range(1, len(episode_rewards) + 1)\n",
    "rewards = episode_rewards\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episodes, rewards)\n",
    "plt.title('Episode Rewards over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum reward achieved: {np.max(rewards):.2f}\")\n",
    "print(f\"Minimum reward achieved: {np.min(rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trained agent in the environment, turn inference mode on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def visualize_policy(agent, env, max_steps=200):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.action(state, inference=True)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        env.render()  # This will display the environment in a separate window\n",
    "        time.sleep(0.01)  # Add a small delay to make the visualization visible\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "# Create the environment with 'human' render mode\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "# Run the visualization\n",
    "final_reward = visualize_policy(agent, env)\n",
    "print(f\"Final episode reward: {final_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Please check out the originl papers and our sources for more information!\n",
    "\n",
    "https://arxiv.org/abs/1801.01290\n",
    "\n",
    "https://arxiv.org/abs/1812.05905\n",
    "\n",
    "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
    "\n",
    "https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac\n",
    "\n",
    "https://github.com/pranz24/pytorch-soft-actor-critic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs8803drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
