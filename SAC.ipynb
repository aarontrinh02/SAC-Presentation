{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC)\n",
    "Soft Actor-Critic (SAC) is a reinforcement learning algorithm that combines an actor-critic architecture with a maximum entropy framework. It is designed to be sample-efficient and stable, making it suitable for complex, continuous action spaces.\n",
    "\n",
    "Key features of SAC:\n",
    "\n",
    "1. Off-policy learning: Can learn from previously collected experiences.\n",
    "2. Actor-Critic structure: Uses both policy and value function approximations.\n",
    "3. Entropy maximization: Encourages exploration and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Entropy\n",
    "Entropy is a measure of randomness or uncertainty in a probability distribution. It is given by:\n",
    "\n",
    "$$\n",
    "H(p) = -\\int p(x) \\log p(x) dx = \\mathbb{E}_{x \\sim p} [-\\log {p(x)}]\n",
    "$$\n",
    "\n",
    "In the context of reinforcement learning, entropy is used to measure the uncertainty or randomness in the policy. By maximizing entropy, the algorithm encourages exploration and prevents premature convergence to suboptimal policies.\n",
    "\n",
    "We augment the reward with an entropy term to encourage exploration.\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{s_t \\sim p(s_t), a_t \\sim \\pi(\\cdot|s_t)} [r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))]\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is a constant that controls the trade-off between the reward and the entropy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Network\n",
    "\n",
    "**Network Architecture is as mentioned in the paper\n",
    "\n",
    "SAC uses the clipped double Q-network trick to provide an estimate of the expected return under the current policy and the target policy.\n",
    "\n",
    "Additionally, SAC uses a target network, an exponential moving average of the critic network, to provide a more stable estimate of the expected return. It is updated slowly as a weighted average of the target and critic network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x1 = self.q1(torch.cat([state, action], 1))\n",
    "        x2 = self.q2(torch.cat([state, action], 1))\n",
    "        return x1, x2\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Network\n",
    "\n",
    "**Network Architecture is as mentioned in the paper\n",
    "\n",
    "The policy network outputs the mean and log standard deviation of the Gaussian distribution over the actions. The mean is the expected action, and the standard deviation controls the uncertainty or randomness in the action distribution.\n",
    "\n",
    "When sampling actions from the policy network, we use the reparameterization trick to allow for gradient descent on the policy network.\n",
    "\n",
    "We use the tanh function to bound the action space to a finite interval (as stated in the original paper).\n",
    "\n",
    "Thus, as shown in the paper, the log probability of the action is given by:\n",
    "\n",
    "$$\n",
    "\\log \\pi(\\mathbf{a}|\\mathbf{s}) = \\log \\mu(\\mathbf{u}|\\mathbf{s}_t) - \\sum_{i=1}^D \\log (1 - \\tanh^2({u}_i))\n",
    "$$\n",
    "\n",
    "Additionally, we scale the action space to ensure that the output of the policy network is within the range of the action space.\n",
    "\n",
    "When returning the scaled action, we have:\n",
    "$$ P(\\cdot | \\pi_\\theta(s)) = ((\\mathcal{N}(tanh(\\mu_{\\theta}(s)), \\sigma_{\\theta}(s)) ) / 2.0) \\times (h - l) + l  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SIG_MIN = -20\n",
    "LOG_SIG_MAX = 2\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        self.action_scale = torch.tensor((action_space.high - action_space.low) / 2.)\n",
    "        self.action_bias = torch.tensor((action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.mean(state)\n",
    "        log_std = self.log_std(state)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        action = normal.rsample() #rsample() samples from the normal distribution using the reparameterization trick\n",
    "        compressed_action = torch.tanh(action)\n",
    "        scaled_action = compressed_action * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(action)\n",
    "        #We add a small constant to the log probability to prevent numerical instability\n",
    "        log_prob = log_prob - torch.log(self.action_scale * (1 - compressed_action.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return scaled_action, log_prob, mean\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super().to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic\n",
    "**Hyperparameters are set as mentioned in the paper\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.005\n",
    "\n",
    "(alpha is initially set arbitrarily to 0.5 since it is learned)\n",
    "\n",
    "network architecture: 2 hidden layers with 256 units each\n",
    "\n",
    "optmiizer: Adam\n",
    "\n",
    "learning rate: 3e-4\n",
    "\n",
    "During each gradient step, we update the parameters by calculating the loss as follows (updating gradients using torch backpropagation):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(Q) = \\mathbb{E}_{s_t, a_t, r_{t+1}, s_{t+1}, d \\sim \\mathcal{D}} [(Q(s_t, a_t) - (r + \\gamma (1-d) (\\text{min}Q_{target}(s_{t+1}, \\pi(s_{t+1}))-\\alpha \\log \\pi(a_{t+1}|s_{t+1}))))^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\pi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon_t \\sim \\mathcal{N}} [\\alpha \\log \\pi(a_t|s_t) - \\text{min}Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\alpha) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} [-\\alpha \\log \\pi(a_t|s_t) - \\alpha \\mathcal{H}_0]\n",
    "$$\n",
    "\n",
    "For the target networks, we use the following update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{Q'} = \\tau \\theta_{Q} + (1-\\tau) \\theta_{Q'}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, num_inputs, action_space, hidden_dim):\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_dim)\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_dim)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.policy = PolicyNetwork(num_inputs, action_space.shape[0], hidden_dim, action_space)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.5\n",
    "        self.target_entropy = -torch.prod(torch.Tensor(action_space.shape[0]).to(device)).item()\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "    \n",
    "\n",
    "    #When in inference mode, we just return the mean since we do not want to incorporate randomness\n",
    "    def action(self, state, inference=False):\n",
    "        if isinstance(state, (int, float)):\n",
    "            state = torch.FloatTensor([state]).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "        if inference:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        else:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        return action.detach().numpy()[0]\n",
    "    \n",
    "    def update(self, memory, batch_size):\n",
    "        states, actions, next_states, rewards, dones = memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "        dones = torch.FloatTensor(dones).to(device).unsqueeze(1)\n",
    "        \n",
    "        #update critic\n",
    "        #We use torch.no_grad() here since the target network does not require gradients\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_pi, _ = self.policy.sample(next_states)\n",
    "            q1_target, q2_target = self.critic_target(next_states, next_action)\n",
    "            q_target = torch.min(q1_target, q2_target) - self.alpha * next_log_pi\n",
    "            next_target_Q = rewards + (dones * self.gamma * (q_target))\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        q1_loss = F.mse_loss(q1, next_target_Q)\n",
    "        q2_loss = F.mse_loss(q2, next_target_Q)\n",
    "        q_loss = q1_loss + q2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        #update policy\n",
    "        pi, log_pi, _ = self.policy.sample(states)\n",
    "        q1_pi, q2_pi = self.critic(states, pi)\n",
    "        min_q_pi = torch.min(q1_pi, q2_pi)\n",
    "        \n",
    "        policy_loss = (self.alpha * log_pi - min_q_pi).mean()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        #update alpha\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optim.step()\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        #update target network\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "The replay buffer is used to store past experiences. We use a circular buffer to store the experiences, ejecting old data when the buffer is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, next_state, reward, done)\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, next_state, reward, done = map(np.stack, zip(*batch))\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "**Hyperparameters are set as mentioned in the paper.\n",
    "\n",
    "buffer_size = 1000000\n",
    "batch_size = 256\n",
    "\n",
    "We use the Pendulum-v1 environment from OpenAI Gym.\n",
    "\n",
    "When training, sample an action from the policy network, and add the experience to the replay buffer. If the replay buffer has enough experiences, sample a batch of experiences from the replay buffer, and update the critic network, policy network, and entropy coefficient. Then, add the new data to the replay buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "env.reset(seed=1)\n",
    "\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, 64)\n",
    "memory = ReplayBuffer(1000000, 1)\n",
    "batch_size = 256\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(200):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "        action = agent.action(state, inference=False)\n",
    "        if len(memory) > batch_size:\n",
    "            agent.update(memory, batch_size)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "\n",
    "        memory.add(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode {i}: | Episode Steps {episode_steps} | Episode Reward {episode_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rewards over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episodes = range(1, len(episode_rewards) + 1)\n",
    "rewards = episode_rewards\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episodes, rewards)\n",
    "plt.title('Episode Rewards over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum reward achieved: {np.max(rewards):.2f}\")\n",
    "print(f\"Minimum reward achieved: {np.min(rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trained agent in the environment, turn inference mode on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def visualize_policy(agent, env, max_steps=200):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.action(state, inference=True)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        env.render()  # This will display the environment in a separate window\n",
    "        time.sleep(0.01)  # Add a small delay to make the visualization visible\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "# Create the environment with 'human' render mode\n",
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "# Run the visualization\n",
    "final_reward = visualize_policy(agent, env)\n",
    "print(f\"Final episode reward: {final_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs8803drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
