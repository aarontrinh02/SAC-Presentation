{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC++ (Soft Actor-Critic++)\n",
    "\n",
    "SAC (Soft Actor-Critic) is a widely-used algorithm in reinforcement learning, building upon the **off-policy** DDPG (Deep Deterministic Policy Gradient) framework discussed in our writing assignment. Unlike DDPG, SAC incorporates **entropy regularization** and trains a **stochastic policy** instead of a deterministic one.\n",
    "\n",
    "To further help understand the concept in SAC, we briefly introduce the entropy idea. \n",
    "\n",
    "Entropy, denoted as \n",
    "$$\n",
    "H(P) = -\\int P(x) \\log P(x) \\, dx = \\mathbb{E}_x[-\\log P(x)],\n",
    "$$ \n",
    "measures the randomness of the policy's action distribution $P(\\cdot)$. Intuitively, higher entropy implies more exploration, while lower entropy indicates more exploitation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **Uniform Distribution (High Entropy):**\n",
    "  \n",
    "  Consider a discrete distribution with 8 equally likely states:\n",
    "  $$\n",
    "  P = \\left(\\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{1}{8}\\right)\n",
    "  $$\n",
    "  The entropy is:\n",
    "  $$\n",
    "  H(P) = -\\sum_{i=1}^{8} P(x_i) \\log_2 P(x_i) = -8 \\times \\frac{1}{8} \\log_2 \\frac{1}{8} = 3 \\text{ bits}\n",
    "  $$\n",
    "  \n",
    "- **Skewed Distribution (Lower Entropy):**\n",
    "  \n",
    "  Now, consider a more deterministic distribution:\n",
    "  $$\n",
    "  P = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}, \\frac{1}{64}\\right)\n",
    "  $$\n",
    "  The entropy decreases to:\n",
    "  $$\n",
    "  H(P) = -\\frac{1}{2} \\log_2 \\frac{1}{2} - \\frac{1}{4} \\log_2 \\frac{1}{4} - \\frac{1}{8} \\log_2 \\frac{1}{8} - \\frac{1}{16} \\log_2 \\frac{1}{16} - 4 \\times \\frac{1}{64} \\log_2 \\frac{1}{64} = 2 \\text{ bits}\n",
    "  $$\n",
    "  \n",
    "As demonstrated, entropy diminishes as the distribution becomes more deterministic.\n",
    "\n",
    "> **Note:** In continuous action spaces, the Gaussian distribution maximizes entropy.\n",
    "\n",
    "In SAC, the reward expectation is modified to include the entropy term:\n",
    "$$\n",
    "r_t + \\gamma (1 - d_t) \\left(Q_{\\phi'} + \\alpha H(P)\\right),\n",
    "$$ \n",
    "where $\\alpha$ is the **entropy temperature** parameter that balances exploration and exploitation. This adjustment ensures that even if the estimated Q-value increases, significant reductions in entropy can prevent premature convergence to suboptimal policies.\n",
    "\n",
    "**Concrete Example:**\n",
    "\n",
    "- **Stochastic Policy:**\n",
    "  \n",
    "  Suppose a policy generates actions with probabilities:\n",
    "  $$\n",
    "  P(a_1) = 0.5, \\quad P(a_2) = 0.5\n",
    "  $$\n",
    "  This distribution has higher entropy, promoting exploration between actions $a_1$ and $a_2$.\n",
    "  \n",
    "- **Deterministic Policy:**\n",
    "  \n",
    "  Alternatively, if the policy generates actions with:\n",
    "  $$\n",
    "  P(a_1) = 0.9, \\quad P(a_2) = 0.1,\n",
    "  $$\n",
    "  the entropy is lower, indicating a preference for action $a_1$ and less exploration.\n",
    "\n",
    "In SAC, we utilize a **continuous action space** with a **Gaussian policy** instead of discrete actions as in DDPG. This approach allows for more nuanced and precise action selections, enhancing exploration in complex environments. Follow the coding box and instructions below to complete this SAC implementation.\n",
    "\n",
    "For more details on DDPG, refer to the original paper [here](https://arxiv.org/pdf/1509.02971).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent from overflows, I apply double precision for float operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentitate the use case from our basic coding questions, we are using Pendulum task with continuous action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike DDPG, SAC trains a stochastic policy $ \\pi_{\\theta}(\\cdot) $ (where $ \\theta $ is parameters) instead of a deterministic policy $ \\mu_{\\theta}(\\cdot) $. <br>\n",
    "Note : For the bounded continuous action space between $ l $ and $ h $, use Gaussian distribution as follows.<br>\n",
    "$$ P(\\cdot | \\pi_\\theta(s)) = ((\\mathcal{N}(tanh(\\mu_{\\theta}(s)), \\sigma_{\\theta}(s)) + 1.0) / 2.0) \\times (h - l) + l  $$\n",
    "Based on the mathematical form above, please implement the GaussianPolicy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Policy net (pi_theta)\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        #############################################################################################\n",
    "        # TODO: Design the network architecture for mu (mean) and sigma (standard deviation), respectively.\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.log_std = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #############################################################################################\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "        #############################################################################################\n",
    "        # TODO: action rescaling based on action_space\n",
    "        # Hint: self.action_scale = ??\n",
    "        # Hint: self.action_bias = ??\n",
    "        \n",
    "        self.action_scale = torch.tensor((env.action_space.high - env.action_space.low) / 2.)\n",
    "        self.action_bias = torch.tensor((env.action_space.high + env.action_space.low) / 2.)\n",
    "        #############################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        #############################################################################################\n",
    "        # TODO: Complete forward function\n",
    "        # Hint: Remember to use LOG_SIG_MIN and LOG_SIG_MAX for log clipping\n",
    "        mean = self.mean(state)\n",
    "        log_std = self.log_std(state)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        \n",
    "        #############################################################################################\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        #############################################################################################\n",
    "        # TODO: Complete sample function given state\n",
    "        # TODO: Return sampled action, log_prob (Derive it from Gaussian Policy), mean (of the Gaussian)\n",
    "        # Hint: For reparameterization trick (mean + std * N(0,1))\n",
    "        # Hint: Remember to Enforcing Action Bound using self.action_scale, self.action_bias\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        sampled_action = normal.rsample(mean.shape)\n",
    "        compressed_action = torch.tanh(sampled_action)\n",
    "        action = compressed_action * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(sampled_action) - torch.log(self.action_scale * (1 - compressed_action.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        \n",
    "        #############################################################################################\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        # If you are using self.action_scale, self.action_bias for action rescaling\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)\n",
    "\n",
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "HIDDEN_DIM = 8\n",
    "actor_lr = 0.1\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "policy = GaussianPolicy(num_inputs, num_actions, hidden_dim=HIDDEN_DIM, action_space=env.action_space).to(device)\n",
    "policy_optim = torch.optim.AdamW(policy.parameters(), actor_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning Introduction\n",
    "\n",
    "In traditional Q-learning, a single Q-function is used to estimate the value of state-action pairs, which can often lead to overestimation bias during training. Double Q-learning addresses this by maintaining two separate Q-networks - $ Q_{\\phi_1}(s), Q_{\\phi_2}(s) $ - to estimate Q-values more accurately. The key idea is that, instead of relying on a single network to both select and evaluate actions, we alternate between the two networks. At each time step, one network selects the action, and the other evaluates its value, reducing overestimation bias.\n",
    "\n",
    "To further enhance stability, we also utilize two target networks - $ Q_{\\phi_1^{\\prime}}(s), Q_{\\phi_2^{\\prime}}(s) $ - to generate stable target values for updates. The target value is computed by taking the minimum of the two Q-value estimates from the target networks, as follows:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\min\\left(Q_{\\phi_1^{\\prime}}(s', \\pi_{\\phi}(s')), Q_{\\phi_2^{\\prime}}(s', \\pi_{\\phi}(s'))\\right)\n",
    "$$\n",
    "\n",
    "By using two Q-networks and taking the minimum of their estimates, we reduce the risk of bias in our value estimates and ensure more stable learning throughout the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        #############################################################################################\n",
    "        # TODO: Design the 2 networks for Q_1 and Q_2 recording from Double Q-Learning\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(num_inputs + num_actions, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        #############################################################################################\n",
    "        # TODO: Complete forward function\n",
    "        # TODO: Return Q_1(s, a) and Q_2(s, a)\n",
    "        # Hint: Simple concatenation should work fine: xu = torch.cat([state, action], 1)\n",
    "        if state.dim() == 2 and action.dim() == 2:\n",
    "            x1 = self.q1(torch.cat([state, action], dim=1))\n",
    "            x2 = self.q2(torch.cat([state, action], dim=1))\n",
    "        elif state.dim() == 3 and action.dim() == 3:\n",
    "            x1 = self.q1(torch.cat([state, action], dim=2))\n",
    "            x2 = self.q2(torch.cat([state, action], dim=2))\n",
    "        else:\n",
    "            raise ValueError(f\"Incompatible dimensions: state {state.shape}, action {action.shape}\")\n",
    "        \n",
    "        #############################################################################################\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "HIDDEN_DIM = 8\n",
    "critic_lr = 0.1\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "critic = QNetwork(num_inputs, num_actions, hidden_dim=HIDDEN_DIM).to(device)  # Q_phi\n",
    "critic_target = QNetwork(num_inputs, num_actions, hidden_dim=HIDDEN_DIM).to(device)  # Q_phi'\n",
    "_ = critic_target.requires_grad_(False)  # target model doen't need grad\n",
    "critic_optim = torch.optim.AdamW(critic.parameters(), critic_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Action Space Entropy Computation\n",
    "\n",
    "In reinforcement learning, when optimizing policies in continuous action spaces, we aim to maximize both the expected reward and the entropy of the policy. Entropy encourages exploration by maximizing the uncertainty of the policy. The optimization objective for the policy parameters $\\theta$ can be formulated as:\n",
    "\n",
    "$$\n",
    "E\\left[ \\pi_\\theta(s) \\cdot Q_{\\phi_1}(s) - \\alpha \\cdot H(P(\\cdot | \\pi_\\theta(s))) \\right]\n",
    "$$\n",
    "\n",
    "Where $H(P)$ is the entropy term, and $\\alpha$ is the temperature parameter controlling the trade-off between the reward and the entropy.\n",
    "\n",
    "#### Entropy in Continuous Action Spaces:\n",
    "For continuous action spaces, the policy $\\pi_\\theta(s)$ is often modeled as a probability distribution such as a Gaussian, where the entropy computation involves integrating over the action space. The entropy of a continuous distribution $\\pi_\\theta(s)$ is given by:\n",
    "\n",
    "$$\n",
    "H(\\pi_\\theta) = - \\int \\pi_\\theta(a | s) \\log \\pi_\\theta(a | s) \\, da\n",
    "$$\n",
    "\n",
    "In the case of a Gaussian policy with mean $\\mu(s)$ and standard deviation $\\sigma(s)$, the entropy has an analytical solution. For a Gaussian distribution, the entropy is:\n",
    "\n",
    "$$\n",
    "H(\\pi_\\theta(s)) = \\frac{1}{2} \\log(2\\pi e \\sigma^2(s))\n",
    "$$\n",
    "\n",
    "This entropy term encourages exploration in continuous action spaces, where the uncertainty of the policy is represented by the variance $\\sigma^2(s)$.\n",
    "\n",
    "#### Optimization Objective:\n",
    "The policy parameters $\\theta$ are optimized to maximize both the expected Q-value and the entropy:\n",
    "\n",
    "$$\n",
    "E\\left[ \\pi_\\theta(s) \\cdot (Q_{\\phi_1}(s) - \\alpha \\log \\pi_\\theta(s)) \\right]\n",
    "$$\n",
    "\n",
    "This equation captures the log-probability of the action, which can be used to compute the entropy and incorporate it into the optimization process.\n",
    "\n",
    "#### Temperature Parameter $\\alpha$:\n",
    "The temperature parameter $\\alpha$ balances the trade-off between maximizing the expected reward $Q_{\\phi_1}(s)$ and the entropy term. In some variants of the Soft Actor-Critic (SAC) algorithm, $\\alpha$ is also learned during training to ensure the entropy is appropriately weighted, allowing for dynamic adjustments as the policy improves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "alpha = 0.1\n",
    "#############################################################################################\n",
    "\n",
    "def optimize_theta(states, tuned_alpha):\n",
    "    #############################################################################################\n",
    "    # TODO: Update Actor\n",
    "    # Hint: Policy Loss: JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "    action, log_prob, mean = policy.sample(states)\n",
    "    action = action.squeeze(-1)\n",
    "    q1, q2 = critic(states, action)\n",
    "    q = torch.min(q1, q2)\n",
    "    policy_loss = (tuned_alpha * log_prob - q).mean()\n",
    "    policy_optim.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optim.step()\n",
    "    #############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we saw in clipped double-Q DDPG, we optimize parameter $ \\phi_1, \\phi_2 $ as follows :\n",
    "\n",
    "- Optimize $ \\phi_1 $ to minimize $ E\\left[ \\left( Q_{\\phi_1}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\left( \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) + \\alpha H(P(\\cdot | \\pi_\\theta(s_{t+1}))) \\right) \\right) \\right)^2 \\right] $\n",
    "- Optimize $ \\phi_2 $ to minimize $ E\\left[ \\left( Q_{\\phi_2}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\left( \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) + \\alpha H(P(\\cdot | \\pi_\\theta(s_{t+1}))) \\right) \\right) \\right)^2 \\right] $\n",
    "\n",
    "in which :\n",
    "\n",
    "- $ Q_{\\phi_i}(s_t, a_t) = Q_{\\phi_i}(s_t) \\cdot \\tilde{a_t} $ where $ \\tilde{a_t} $ is one hot vector of $ a_t $\n",
    "- $ Q_{{\\phi_i}^{\\prime}}(s_{t+1},a^*_{t+1}) =  Q_{\\phi_i^{\\prime}}(s_{t+1}) \\cdot \\pi_\\theta(s_{t+1}) $ where $ \\pi_\\theta(s_{t+1}) $ is one hot probability\n",
    "- $ H(P(\\cdot | \\pi_\\theta(s_{t+1}))) = -\\pi_\\theta(s_{t+1}) \\cdot \\log \\pi_\\theta(s_{t+1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "gamma = 0.99\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def optimize_phi(states, actions, rewards, next_states, dones, tuned_alpha):\n",
    "    #############################################################################################\n",
    "    # TODO: Update Critic\n",
    "    # Hint: Convert everything to tensor first\n",
    "    # Hint: Use double Q-functions to mitigate positive bias in the policy improvement step\n",
    "    # Hint: Compute target Q value: r + gamma * (1 - dones) (min Q(s_next,a_next') + alpha * H(P))\n",
    "    # Hint: Q1_loss: JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float64)\n",
    "    actions = torch.tensor(actions, dtype=torch.float64)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float64)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float64)\n",
    "    dones = torch.tensor(dones, dtype=torch.float64)\n",
    "    action, log_prob, mean = policy.sample(next_states)\n",
    "    entropy = -log_prob\n",
    "    q1, q2 = critic(states, actions)\n",
    "    q1_prime, q2_prime = critic_target(next_states, action)\n",
    "    q_prime = torch.min(q1_prime, q2_prime)\n",
    "    target_q = rewards + gamma * (1 - dones) * (q_prime + tuned_alpha * entropy)\n",
    "    q1_loss = (q1 - target_q).pow(2).mean()\n",
    "    q2_loss = (q2 - target_q).pow(2).mean()\n",
    "    critic_optim.zero_grad()\n",
    "    q1_loss.backward()\n",
    "    q2_loss.backward()\n",
    "    critic_optim.step()\n",
    "        \n",
    "    #############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in clipped double-Q DDPG, target parameters $\\phi_1^{\\prime}, \\phi_2^{\\prime}$ are delayed with coefficient parameter (hyper-parameter) $ \\tau $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyper-parameters Tuning ##################################\n",
    "# Hint: smaller!\n",
    "tau = 0.2\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(critic.parameters(), critic_target.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the basic coding questions, we use replay buffer to prevent from learning only for recent experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "# Hint: Try larger size\n",
    "buffer_size = 200\n",
    "#############################################################################################\n",
    "\n",
    "class replayBuffer:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(item)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        #############################################################################################\n",
    "        # TODO: Sample within ReplayBuffer\n",
    "        # Hint: Refer to implementation in src/buffer.py\n",
    "        \n",
    "        sampled_data = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, n_states, dones = zip(*sampled_data)\n",
    "        states = torch.tensor(states, dtype=torch.float64)\n",
    "        actions = torch.tensor(actions, dtype=torch.float64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float64)\n",
    "        n_states = torch.tensor(n_states, dtype=torch.float64)\n",
    "        dones = torch.tensor(dones, dtype=torch.float64)\n",
    "        \n",
    "        #############################################################################################\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "buffer = replayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e210da",
   "metadata": {},
   "source": [
    "In SAC++, the modification is the automatic tuning of the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5256271",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_LR = 0.1\n",
    "\n",
    "entropy_target = -torch.prod(torch.tensor(env.action_space.shape)) #target = -|A|\n",
    "log_alpha = torch.zeros(1, requires_grad=True).to(device)\n",
    "log_alpha_optim = torch.optim.Adam([log_alpha], lr=ALPHA_LR)\n",
    "\n",
    "def optimize_alpha(states, actions, rewards, next_states, dones):\n",
    "    action, log_prob, mean = policy.sample(states)\n",
    "    alpha_loss = -(log_alpha * (log_prob + entropy_target).detach()).mean()\n",
    "    log_alpha_optim.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    log_alpha_optim.step()\n",
    "    alpha = log_alpha.exp()\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put all we have together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimensions: state torch.Size([50, 3]), action torch.Size([50, 1, 50])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mlength() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#############################################################################################\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# TODO: If enough data, sample data from ReplayBuffer and update Actor-Critic\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Hint: remember to soft update target using update_target()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     states, actions, rewards, n_states, dones \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 27\u001b[0m     \u001b[43moptimize_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     optimize_phi(states, actions, rewards, n_states, dones)\n\u001b[1;32m     29\u001b[0m     update_target()\n",
      "Cell \u001b[0;32mIn[86], line 12\u001b[0m, in \u001b[0;36moptimize_theta\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m     10\u001b[0m action, log_prob, mean \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39msample(states)\n\u001b[1;32m     11\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m q1, q2 \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1, q2)\n\u001b[1;32m     14\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m (alpha \u001b[38;5;241m*\u001b[39m log_prob \u001b[38;5;241m-\u001b[39m q)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs8803drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs8803drl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 45\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq2(torch\u001b[38;5;241m.\u001b[39mcat([state, action], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions: state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, action \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#############################################################################################\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x1, x2\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimensions: state torch.Size([50, 3]), action torch.Size([50, 1, 50])"
     ]
    }
   ],
   "source": [
    "################################## Hyper-parameters Tuning ##################################\n",
    "\n",
    "# Hint: Try larger size\n",
    "batch_size = 50\n",
    "#############################################################################################\n",
    "\n",
    "reward_records = []\n",
    "for i in range(1000):\n",
    "    # Run episode till done\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        #############################################################################################\n",
    "        # TODO: Sample enough data from policy and push to ReplayBuffer\n",
    "        action, log_prob, mean = policy.sample(torch.tensor(s, dtype=torch.float64).unsqueeze(0))\n",
    "        s_, reward, done, _, _ = env.step(action.squeeze().detach().numpy().reshape(1))\n",
    "        buffer.add((s, action, reward, s_, done))\n",
    "        #############################################################################################\n",
    "        \n",
    "        cum_reward += reward\n",
    "        if buffer.length() >= batch_size:\n",
    "            #############################################################################################\n",
    "            # TODO: If enough data, sample data from ReplayBuffer and update Actor-Critic\n",
    "            # Hint: remember to soft update target using update_target()\n",
    "            states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
    "            optimize_theta(states)\n",
    "            optimize_phi(states, actions, rewards, n_states, dones)\n",
    "            alpha = optimize_alpha(states, actions, rewards, n_states, dones)\n",
    "            update_target()\n",
    "            #############################################################################################\n",
    "\n",
    "    # Output total rewards in episode (max 500)\n",
    "    print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\r\")\n",
    "    reward_records.append(cum_reward)\n",
    "\n",
    "    # stop if reward mean > -150.0\n",
    "    if np.average(reward_records[-50:]) > -150.0:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and tuning, save the SAC_returns plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx + 1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx - 49:idx + 1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "\n",
    "# Plot both reward records and average rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reward_records, label='Record Reward', color='blue')\n",
    "plt.plot(average_reward, label='Average Reward (50 Epochs)', color='orange', linestyle='--')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Soft Actor-Critic', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Cumulative Rewards', fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('sac_returns.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are welcome to explore different continuous action environments by simply changing the environment code (might need additional package installation), for example:\n",
    "\n",
    "```python\n",
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "```\n",
    "We encourage you to write your own evaluation function and save the simulation visualization as a GIF file. This will not be graded, but it's fun to try.\n",
    "\n",
    "Here are some potential complex tasks you can explore:  [RL environment list](https://github.com/clvrai/awesome-rl-envs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
